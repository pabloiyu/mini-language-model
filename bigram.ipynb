{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use my already existing python environment, I had to give Visual Studio Code the path to my environments folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394 characters.\n",
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of dataset: {len(text)} characters.\")\n",
    "\n",
    "# There are a total of 65 unique characters in the dataset.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(\"\".join(chars))\n",
    "\n",
    "# We will tokenize our vocabulary by building a character level language model. We will represent each\n",
    "# character as an integer. Sub-word tokenizers are also possible (chat-gpt uses tiktoken)\n",
    "# We first create a mapping from characters to integers using a dictionary\n",
    "chtoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoch = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):  \n",
    "    return [chtoi[ch] for ch in s] # Take a string, output list of integers.\n",
    "\n",
    "def decode(list_int):\n",
    "    return \"\".join([itoch[i] for i in list_int]) # Take a list of integers, output string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now encode entire \"input.txt\" and save it in a torch tensor.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a transformer, we only work with random chunks we take from the dataset. \n",
    "\n",
    "In a chunk of 9 characters, there are 8 training examples of increasing context length. Maximum context length we train with is given by block_size. This is useful for inference as the transformer is used to working with varying context lengths. For inference, we have to divide inputs larger than block_size into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT\n",
      "When tensor([18]) is the context, the target is 47.\n",
      "When tensor([18, 47]) is the context, the target is 56.\n",
      "When tensor([18, 47, 56]) is the context, the target is 57.\n",
      "When tensor([18, 47, 56, 57]) is the context, the target is 58.\n",
      "When tensor([18, 47, 56, 57, 58]) is the context, the target is 1.\n",
      "When tensor([18, 47, 56, 57, 58,  1]) is the context, the target is 15.\n",
      "When tensor([18, 47, 56, 57, 58,  1, 15]) is the context, the target is 47.\n",
      "When tensor([18, 47, 56, 57, 58,  1, 15, 47]) is the context, the target is 58.\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "print(\"CONTEXT\")\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When {context} is the context, the target is {target}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    We obtain a context and target tensor of size (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
    "\n",
    "    # We now turn horizontally\n",
    "    X = torch.vstack([data[i:i+block_size] for i in ix])\n",
    "    Y = torch.vstack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are a very simple model. They simply use a look-up table and no context. They use only the current character to predict the next. \n",
    "\n",
    "The objective of the generate() function is to extend the (batch_size, block_size) horizontally and predict more tokens. Gets (B,T) -> (B,T+1)\n",
    "\n",
    "min 38\n",
    "\n",
    "logit: output of a neuron without applying activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 65])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([[0,4,6,2],\n",
    "                    [3,7,8,9]])  # size: (batch_size, block_size)\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "logits = token_embedding_table(idx) # size: (batch_size, block_size, vocab_size)\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # First input is vocab_size. Second input is the size of the encoded representation for each word. \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"P\n",
    "        Embeddings are used when working with categorical data. Often used to map discrete tokens (such as characters in a text)\n",
    "        to continous vectors.\n",
    "\n",
    "        Useful link.\n",
    "        https://spltech.co.uk/in-pytorch-what-is-nn-embedding-for-and-how-is-it-different-from-one-hot-encding-for-representing-categorical-data/?utm_content=cmp-true\n",
    "        \"\"\"\n",
    "        # idx and targets are tensors of size (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx)   # size: (batch_size, block_size, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        We call this function to generate new characters.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We first get the predictions\n",
    "            logits, loss = self(idx)  # (B,T,C)\n",
    "            \n",
    "            # Here, we are interested in using all the given context.\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "\n",
    "            # We then apply softmax to get probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "\n",
    "            # We now sample from the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "\n",
    "            # Finally, we append\n",
    "            idx = torch.hstack([idx, idx_next])  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0  train_loss: 4.6162  val_loss: 4.6193\n",
      "iter: 1000  train_loss: 2.4922  val_loss: 2.5184\n",
      "iter: 2000  train_loss: 2.4764  val_loss: 2.4926\n",
      "iter: 3000  train_loss: 2.4600  val_loss: 2.4883\n",
      "iter: 4000  train_loss: 2.4560  val_loss: 2.4900\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 2000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(eval_iters)\n",
    "        print(f\"iter: {iter}  train_loss: {losses['train']:.4f}  val_loss: {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "Thou art, t, ine t nghanstl stomy t t ncthever hastind e fr balawin fas, mirengered fap ig' had yocrs.\n",
      "IIncearuld st n,\n",
      "\n",
      "RDort saspe ariasth o towsixf my hin angal der lled\n",
      "S m\n",
      "LUKEThyour s.\n",
      "\n",
      "ARCosarnevik e blode d bllo ttag hir wat? t t s\n",
      "BEO: s o, ad.\n",
      "\n",
      "Cl IIORDomart k'd CENCEMIUS:\n",
      "OMu tr'e;\n",
      "A: the thadswoJULEO scine to: y the. anges, War pETrmevanemy gacouthe st wis.\n",
      "THENSThe; cksimer r ar se, sh HAnt e\n"
     ]
    }
   ],
   "source": [
    "context = \"Thou art\"\n",
    "context = torch.tensor(encode(context)).unsqueeze(0)\n",
    "print(context.shape)\n",
    "\n",
    "text = decode(m.generate(context, 200)[0].tolist())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "b = 1\n",
    "t = 2\n",
    "print(x[b, :t+1, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is to code up the most simple type of attention. Where for each batch independently, for each target t, we take the mean of the previous context. We will refer to this tensor as xbow (bag of words). The name comes from the fact that averaging is essentially just throwing all the words into a bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# x[b,t] = mean_{i <= t} x[b,i]\n",
    "xbow = torch.zeros_like(x)\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1, :] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)\n",
    "print(xbow[0])\n",
    "\n",
    "# There is a way to make this code much more efficient. We can perform this weighted aggregation with matrix multiplication. \n",
    "low_triangular_ones = torch.tril(torch.ones((T,T)))\n",
    "divisor = torch.arange(1, T+1, device=device).unsqueeze(1)\n",
    "low_triangular_ones /= divisor\n",
    "print(low_triangular_ones)\n",
    "\n",
    "# We perform matrix multiplication over each batch independently. We can use einsum for tensor operations.\n",
    "xbow_einsum = torch.einsum('ij, ajk-> aik', low_triangular_ones, x)\n",
    "\n",
    "# For matmul we have to introduce intermediate tensors. \n",
    "low_triangular_ones_batch = low_triangular_ones.unsqueeze(0).repeat(B, 1, 1)\n",
    "xbow_matmul = torch.matmul(low_triangular_ones_batch , x)\n",
    "\n",
    "print(torch.allclose(xbow, xbow_einsum))\n",
    "print(torch.allclose(xbow, xbow_matmul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
