{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use my already existing python environment, I had to give Visual Studio Code the path to my environments folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394 characters.\n",
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of dataset: {len(text)} characters.\")\n",
    "\n",
    "# There are a total of 65 unique characters in the dataset.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(\"\".join(chars))\n",
    "\n",
    "# We will tokenize our vocabulary by building a character level language model. We will represent each\n",
    "# character as an integer. Sub-word tokenizers are also possible (chat-gpt uses tiktoken)\n",
    "# We first create a mapping from characters to integers using a dictionary\n",
    "chtoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoch = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):  \n",
    "    return [chtoi[ch] for ch in s] # Take a string, output list of integers.\n",
    "\n",
    "def decode(list_int):\n",
    "    return \"\".join([itoch[i] for i in list_int]) # Take a list of integers, output string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now encode entire \"input.txt\" and save it in a torch tensor.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a transformer, we only work with random chunks we take from the dataset. \n",
    "\n",
    "In a chunk of 9 characters, there are 8 training examples of increasing context length. Maximum context length we train with is given by block_size. This is useful for inference as the transformer is used to working with varying context lengths. For inference, we have to divide inputs larger than block_size into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT\n",
      "When tensor([18]) is the context, the target is 47.\n",
      "When tensor([18, 47]) is the context, the target is 56.\n",
      "When tensor([18, 47, 56]) is the context, the target is 57.\n",
      "When tensor([18, 47, 56, 57]) is the context, the target is 58.\n",
      "When tensor([18, 47, 56, 57, 58]) is the context, the target is 1.\n",
      "When tensor([18, 47, 56, 57, 58,  1]) is the context, the target is 15.\n",
      "When tensor([18, 47, 56, 57, 58,  1, 15]) is the context, the target is 47.\n",
      "When tensor([18, 47, 56, 57, 58,  1, 15, 47]) is the context, the target is 58.\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "print(\"CONTEXT\")\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When {context} is the context, the target is {target}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    We obtain a context and target tensor of size (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
    "\n",
    "    # We now turn horizontally\n",
    "    X = torch.vstack([data[i:i+block_size] for i in ix])\n",
    "    Y = torch.vstack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are a very simple model. They simply use a look-up table and no context. They use only the current character to predict the next. \n",
    "\n",
    "The objective of the generate() function is to extend the (batch_size, block_size) horizontally and predict more tokens. Gets (B,T) -> (B,T+1)\n",
    "\n",
    "min 38\n",
    "\n",
    "logit: output of a neuron without applying activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 65])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor([[0,4,6,2],\n",
    "                    [3,7,8,9]])  # size: (batch_size, block_size)\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "logits = token_embedding_table(idx) # size: (batch_size, block_size, vocab_size)\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # First input is vocab_size. Second input is the size of the encoded representation for each word. \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"P\n",
    "        Embeddings are used when working with categorical data. Often used to map discrete tokens (such as characters in a text)\n",
    "        to continous vectors.\n",
    "\n",
    "        Useful link.\n",
    "        https://spltech.co.uk/in-pytorch-what-is-nn-embedding-for-and-how-is-it-different-from-one-hot-encding-for-representing-categorical-data/?utm_content=cmp-true\n",
    "        \"\"\"\n",
    "        # idx and targets are tensors of size (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx)   # size: (batch_size, block_size, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        We call this function to generate new characters.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We first get the predictions\n",
    "            logits, loss = self(idx)  # (B,T,C)\n",
    "            \n",
    "            # Here, we are interested in using all the given context.\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "\n",
    "            # We then apply softmax to get probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "\n",
    "            # We now sample from the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "\n",
    "            # Finally, we append\n",
    "            idx = torch.hstack([idx, idx_next])  # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0  train_loss: 4.6162  val_loss: 4.6193\n",
      "iter: 1000  train_loss: 2.4922  val_loss: 2.5184\n",
      "iter: 2000  train_loss: 2.4764  val_loss: 2.4926\n",
      "iter: 3000  train_loss: 2.4600  val_loss: 2.4883\n",
      "iter: 4000  train_loss: 2.4560  val_loss: 2.4900\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 2000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(eval_iters)\n",
    "        print(f\"iter: {iter}  train_loss: {losses['train']:.4f}  val_loss: {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "Thou art, t, ine t nghanstl stomy t t ncthever hastind e fr balawin fas, mirengered fap ig' had yocrs.\n",
      "IIncearuld st n,\n",
      "\n",
      "RDort saspe ariasth o towsixf my hin angal der lled\n",
      "S m\n",
      "LUKEThyour s.\n",
      "\n",
      "ARCosarnevik e blode d bllo ttag hir wat? t t s\n",
      "BEO: s o, ad.\n",
      "\n",
      "Cl IIORDomart k'd CENCEMIUS:\n",
      "OMu tr'e;\n",
      "A: the thadswoJULEO scine to: y the. anges, War pETrmevanemy gacouthe st wis.\n",
      "THENSThe; cksimer r ar se, sh HAnt e\n"
     ]
    }
   ],
   "source": [
    "context = \"Thou art\"\n",
    "context = torch.tensor(encode(context)).unsqueeze(0)\n",
    "print(context.shape)\n",
    "\n",
    "text = decode(m.generate(context, 200)[0].tolist())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "b = 1\n",
    "t = 2\n",
    "print(x[b, :t+1, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do now is to code up the most simple type of attention. Where for each batch independently, for each target t, we take the mean of the previous context. We will refer to this tensor as xbow (bag of words). The name comes from the fact that averaging is essentially just throwing all the words into a bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# x[b,t] = mean_{i <= t} x[b,i]\n",
    "xbow = torch.zeros_like(x)\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1, :] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)\n",
    "print(xbow[0])\n",
    "\n",
    "# There is a way to make this code much more efficient. We can perform this weighted aggregation with matrix multiplication. \n",
    "low_triangular_ones = torch.tril(torch.ones((T,T)))\n",
    "divisor = torch.arange(1, T+1, device=device).unsqueeze(1)\n",
    "low_triangular_ones /= divisor\n",
    "\n",
    "# We can use einsum instead of relying on broadcasting.\n",
    "low_triangular_ones = torch.tril(torch.ones((T,T)))\n",
    "divisor = 1/torch.arange(1, T+1, device=device)\n",
    "low_triangular_ones = torch.einsum('ij, i -> ij', low_triangular_ones, divisor)\n",
    "\n",
    "print(low_triangular_ones)\n",
    "\n",
    "# We perform matrix multiplication over each batch independently. We can use einsum for tensor operations.\n",
    "xbow_einsum = torch.einsum('ij, ajk-> aik', low_triangular_ones, x)\n",
    "\n",
    "# For matmul we do batch multiplication.\n",
    "xbow_matmul = low_triangular_ones@ x\n",
    "\n",
    "print(torch.allclose(xbow, xbow_einsum))\n",
    "print(torch.allclose(xbow, xbow_matmul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# The @ operator in PyTorch performs batch matrix multiplication when the input tensors have compatible shapes\n",
    "\n",
    "wei = torch.tril(torch.ones((T,T)))\n",
    "wei /= wei.sum(dim=1, keepdim=True) # keepdim = True stops Pytorch from squeezing the tensor along the dimension we summed over.\n",
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) \n",
    "\n",
    "# There is another identical version that uses softmax\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))  # wei has -inf in all elements above main diagonal. This forbids tokens communicating with future tokens.\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # Equivalent to torch.einsum('ij, ajk -> aik', wei, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's interesting about the above implementation using softmax is that it allows for soft attention. We do not have to initialise wei with torch.zeros((T,T)). We can initialise with different affinities between the tokens. We will aggregate the values depending on how interesting tokens find each other.\n",
    "\n",
    "I want to now gather information from the past in a data-dependent way. This is the problem self-attention solves. Every single token will emit two vectors: a query and a key. \n",
    "\n",
    "    query: what am I looking for             key: what do I contain\n",
    "\n",
    "The way we get affinities is the following. Token t's query vector is dot producted with the key vectors from all previous tokens. This creates the wei matrix. Thus, if the key and the query are aligned, those tokens will interact in a very high amount. Additionally, we don't matrix multiply wei directly with x. We first obtain a value matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)   # function\n",
    "query = nn.Linear(C, head_size, bias=False) # function\n",
    "value = nn.Linear(C, head_size, bias=False) # function\n",
    "\n",
    "# Now every token has a key and a query vector associated to it.\n",
    "k = key(x)    # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "wei = torch.einsum('ijk, ilk -> ijl', q, k) # equivalent\n",
    "\n",
    "wei *= head_size**(-0.5)  # we scale the values to prevent sharpening of wei after softmax\n",
    "\n",
    "# wei now contains an affinity value between all the tokens as we have performed dot products between keys and queries.\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)  # (B, T, head_size)\n",
    "out = wei @ x # (T,T) @ (B, T, head_size) -> (B, T, head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example. Let's say we're the 8th token and we're a vowel. We create a query: \"Hey. I'm a vowel in the 8th position and I'm looking for any consonant at positions up to 4\". Then all past tokens emit their key. Maybe one of the tokens has a key which satisifies those requirements. That key vector would have a high number in the specific channel that represents that requirement, which would create affinity between these two tokens.\n",
    "\n",
    "Let's develop an intuition behind key, query, value.\n",
    "\n",
    "    x: (B, T, C)  Contains private information for each token. Now for the purpose of this single head of self-attention, here is some information about me:\n",
    "\n",
    "    + query: here's what I'm interested in\n",
    "\n",
    "    + key: here's what I have\n",
    "\n",
    "    + value: if you find me interesting, here's what I will communicate to you\n",
    "\n",
    "So v is the thing that gets aggregated for the purpose of this single head.\n",
    "\n",
    "IDEA: As we have vectors, there is no feature interaction like we would get in a Gaussian process. That is, maybe the vowel cares about the consonant being up to the 4th position only if one of of the other channels is close to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES\n",
    "\n",
    "+ Note that there is no notion of space, unlike in convolution where the convolution acts spatially. This is why we need to positionally encode tokens.\n",
    "\n",
    "+ Each example from each batch is processed independently.\n",
    "\n",
    "+ In an encoder attention block we can allow tokens to communicate (simply eliminate masking operation). This is useful for example when we want to perform sentiment analysis of a sentence. What we do with masking is called a decoder attention block, and is usually used in autoregressive settings such as language modelling.\n",
    "\n",
    "+ What we've implemented is called \"self-attention\" because the same source 'x' produces the keys, queries and values. Cross-attention is used when there's a separate source of tokens we wish to pull information from. In encoder-decoder transformers, you can have the case where the queries are produced from 'x' but the keys and values come from a whole separate external source, maybe from encoder blocks that encode some context we want to condition on. \n",
    "\n",
    "+ Scaled attention divides wei by sqrt(head_size). This makes it so that when input Q,K are unit variance, wei will be unit variance too. Softmax will stay diffuse and not saturate too much. Especially at initialisation, if we have very positive and negative values, wei will converge to one-hot encoding. It will sharpen to whatever value happens to be most positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement one head of self-attention in \"transformer.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
